@article{jonas2018lasso,
  title={LASSO regression},
  author={Jonas, R and Cook, J},
  journal={British Journal of Surgery},
  volume={105},
  number={10},
  year={2018},
  publisher={John Wiley \& Sons, Inc.}
}

@article{10.1111/j.2517-6161.1996.tb02080.x,
    author = {Tibshirani, Robert},
    title = {Regression Shrinkage and Selection Via the Lasso},
    journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
    volume = {58},
    number = {1},
    pages = {267-288},
    year = {2018},
    month = {12},
    abstract = {We propose a new method for estimation in linear models. The ‘lasso’ minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
    issn = {0035-9246},
    doi = {10.1111/j.2517-6161.1996.tb02080.x},
    url = {https://doi.org/10.1111/j.2517-6161.1996.tb02080.x},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/58/1/267/49098631/jrsssb\_58\_1\_267.pdf},
}

@article{spencer2018refinement,
  title={A refinement of lasso regression applied to temperature forecasting},
  author={Spencer, Bruce and Alfandi, Omar and Al-Obeidat, Feras},
  journal={Procedia computer science},
  volume={130},
  pages={728--735},
  year={2018},
  publisher={Elsevier}
}
TY  - JOUR
AU  - Fontanarosa, Joel B.
AU  - Dai, Yang
PY  - 2011
DA  - 2011/11/29
TI  - Using LASSO regression to detect predictive aggregate effects in genetic studies
JO  - BMC Proceedings
SP  - S69
VL  - 5
IS  - 9
AB  - We use least absolute shrinkage and selection operator (LASSO) regression to select genetic markers and phenotypic features that are most informative with respect to a trait of interest. We compare several strategies for applying LASSO methods in risk prediction models, using the Genetic Analysis Workshop 17 exome simulation data consisting of 697 individuals with information on genotypic and phenotypic features (smoking, age, sex) in 5-fold cross-validated fashion. The cross-validated averages of the area under the receiver operating curve range from 0.45 to 0.63 for different strategies using only genotypic markers. The same values are improved to 0.69–0.87 when both genotypic and phenotypic information are used. The ability of the LASSO method to find true causal markers is limited, but the method was able to discover several common variants (e.g., FLT1) under certain conditions.
SN  - 1753-6561
UR  - https://doi.org/10.1186/1753-6561-5-S9-S69
DO  - 10.1186/1753-6561-5-S9-S69
ID  - Fontanarosa2011
ER  - 



@Manual{R-base,
  title = {R: A Language and Environment for Statistical
           Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2019},
  url = {https://www.R-project.org},
}

@book{efr2008,
  title={Nonparametric Curve Estimation: Methods, Theory, and Applications},
  author={Efromovich, S.},
  isbn={9780387226385},
  lccn={99013253},
  series={Springer Series in Statistics},
  url={https://books.google.com/books?id=mdoLBwAAQBAJ},
  year={2008},
  publisher={Springer New York}
}
@article{bro2014principal,
  title={Principal component analysis},
  author={Bro, Rasmus and Smilde, Age K},
  journal={Analytical methods},
  volume={6},
  number={9},
  pages={2812--2831},
  year={2014},
  publisher={Royal Society of Chemistry}
}
